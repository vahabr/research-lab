# Reproducible environment for anchored Kalman filtering in a Markov PD model
# NumPy / Pandas / Matplotlib only. Deterministic. One function per task.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# -------------------------
# Utilities / Configuration
# -------------------------

OUTDIR = Path("/mnt/data")
OUTDIR.mkdir(parents=True, exist_ok=True)

RATINGS = ["A", "B", "C", "D"]
IDX = {r:i for i, r in enumerate(RATINGS)}
T_DEFAULT = 20  # quarters
N_DEFAULT = 10_000

# -------------------------
# 1) Determinism
# -------------------------

def set_seed(seed: int) -> np.random.Generator:
    """Set global determinism and return a local RNG."""
    np.random.seed(seed)
    return np.random.default_rng(seed)


# -------------------------
# 2) Portfolio
# -------------------------

def gen_initial_portfolio(N: int, pi0: np.ndarray, rng: np.random.Generator=None) -> np.ndarray:
    """Sample initial ratings for N obligors from distribution pi0."""
    if rng is None:
        rng = np.random.default_rng()
    labels = rng.choice(len(RATINGS), size=N, p=pi0)
    return labels


# -------------------------
# 3) TTC Transition Matrix
# -------------------------

def build_P_TTC() -> np.ndarray:
    """Return the quarterly TTC transition matrix."""
    P_TTC = np.array([
        [0.975, 0.022, 0.002, 0.001],
        [0.030, 0.935, 0.030, 0.005],
        [0.010, 0.060, 0.915, 0.015],
        [0.000, 0.000, 0.000, 1.000],
    ], dtype=float)
    return P_TTC


# -------------------------
# 4) Macroeconomic Scenarios
# -------------------------

def _baseline_paths(T: int) -> tuple[np.ndarray, np.ndarray]:
    """Baseline: mild cycle. GDP in pp q/q, Unemp level in pp."""
    t = np.arange(T)
    gdp = 0.5 + 0.3*np.sin(2*np.pi*t/12)  # around 0.5% q/q, small cycle
    unemp = 5.0 + 0.2*np.cos(2*np.pi*(t+3)/10)  # around 5%, small wiggle
    return gdp, unemp

def _stress_paths(T: int) -> tuple[np.ndarray, np.ndarray]:
    """Stress: sharp downturn, slow recovery."""
    t = np.arange(T)
    gdp = 0.6 - 0.8*np.exp(-t/2.0)  # drop quickly then recover slowly
    gdp[:4] -= 0.8  # accentuate early downturn
    unemp = 4.5 + 1.8*(1 - np.exp(-t/4.0))  # rises then plateaus
    return gdp, unemp

def _pandemic_paths(T: int) -> tuple[np.ndarray, np.ndarray]:
    """Pandemic: abrupt fall, fast rebound (impose rebound at t=3 later)."""
    t = np.arange(T)
    gdp = 0.5*np.ones(T)
    gdp[0:2] = -2.0  # abrupt fall
    gdp[2] = -0.5
    # Rebound will be imposed at t=3 in realised series
    unemp = 4.2*np.ones(T)
    unemp[0:3] = 5.5  # spike
    unemp[3:6] = 5.0
    return gdp, unemp

def gen_macro_forecasts_and_realised(scenario: str, T: int=T_DEFAULT, rng: np.random.Generator=None) -> dict:
    """Return dict with forecast and realised GDP/UNEMP and the macro index paths."""
    if rng is None:
        rng = np.random.default_rng()
    scenario = scenario.lower()
    if scenario == "baseline":
        gdp_f, unemp_f = _baseline_paths(T)
    elif scenario == "stress":
        gdp_f, unemp_f = _stress_paths(T)
    elif scenario in {"pandemic", "pandemic_shock"}:
        gdp_f, unemp_f = _pandemic_paths(T)
    else:
        raise ValueError("Unknown scenario")

    # Realised paths = forecast + Gaussian noise
    # σ_g = 0.2pp GDP q/q, σ_u = 0.2pp unemployment
    gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)
    unemp_r = unemp_f + rng.normal(0.0, 0.2, size=T)

    # Imposed pandemic rebound at t=3 (0-indexed) for pandemic scenario
    if scenario in {"pandemic", "pandemic_shock"} and T >= 4:
        gdp_r[3] += 2.5  # sharp rebound
        unemp_r[3] -= 0.8  # fast improvement

    # Macro indices
    M_hat = macro_index_from_gdp_unemp(gdp_f, unemp_f)
    M_real = macro_index_from_gdp_unemp(gdp_r, unemp_r)

    return {
        "gdp_forecast": gdp_f, "unemp_forecast": unemp_f,
        "gdp_realised": gdp_r, "unemp_realised": unemp_r,
        "M_hat": M_hat, "M_real": M_real
    }


def macro_index_from_gdp_unemp(GDP: np.ndarray, UNEMP: np.ndarray) -> np.ndarray:
    """Return M_t = 0.5*z(GDP) - 0.5*z(UNEMP)."""
    # z-score per input series
    def z(x: np.ndarray) -> np.ndarray:
        mu = np.mean(x)
        sigma = np.std(x, ddof=0)
        sigma = sigma if sigma > 1e-12 else 1.0
        return (x - mu) / sigma
    return 0.5*z(GDP) - 0.5*z(UNEMP)


# -------------------------
# 5) PIT Overlay
# -------------------------

def _build_betas() -> np.ndarray:
    """Beta matrix β_ij with specified nearest-neighbour/default structure."""
    beta = np.zeros((4,4), dtype=float)

    # Downgrades / defaults (given)
    beta[IDX["A"], IDX["B"]] = 2.0
    beta[IDX["A"], IDX["C"]] = 2.5
    beta[IDX["A"], IDX["D"]] = 3.0

    beta[IDX["B"], IDX["C"]] = 1.5
    beta[IDX["B"], IDX["D"]] = 2.0

    beta[IDX["C"], IDX["D"]] = 1.2

    # Upgrades are negatives
    beta[IDX["B"], IDX["A"]] = -beta[IDX["A"], IDX["B"]]
    beta[IDX["C"], IDX["B"]] = -beta[IDX["B"], IDX["C"]]
    beta[IDX["C"], IDX["A"]] = -beta[IDX["A"], IDX["C"]]
    # Diagonals and other moves left at 0.0
    # Default row absorbing: handled in normalisation below
    return beta


def pit_overlay(P_TTC: np.ndarray, M_t: float, betas: np.ndarray) -> np.ndarray:
    """Return PIT transition matrix for given M_t."""
    P = np.array(P_TTC, dtype=float)
    # Apply logit-style tilt
    W = P * np.exp(betas * M_t)
    # Row normalisation, keep D absorbing
    for i in range(W.shape[0]):
        if i == IDX["D"]:
            W[i, :] = 0.0
            W[i, IDX["D"]] = 1.0
        else:
            s = W[i, :].sum()
            if s <= 0:
                # Fallback to TTC row
                W[i, :] = P[i, :]
            else:
                W[i, :] = W[i, :] / s
    return W


# -------------------------
# 6) Kalman Filters
# -------------------------

def kalman_naive(M_hat: np.ndarray, rho: float=0.90, Q: float=None, R: float=0.25) -> np.ndarray:
    """Naïve KF where observation is the forecast M_hat_t (z-units)."""
    T = len(M_hat)
    if Q is None:
        Q = 1 - rho**2  # 0.19
    # State init
    m = 0.0
    P = 1.0
    H = 1.0

    m_filt = np.zeros(T)
    for t in range(T):
        # Predict
        m_pred = rho * m
        P_pred = rho * P * rho + Q

        # Update with y_t = M_hat[t]
        y = M_hat[t]
        S = H * P_pred * H + R
        K = P_pred * H / S
        m = m_pred + K * (y - H * m_pred)
        P = (1 - K * H) * P_pred

        m_filt[t] = m
    return m_filt


def kalman_anchored(M_hat: np.ndarray, T_anchor: int=20, rho: float=0.90, Q: float=None, R: float=0.25, sigma_star2_pre: float=0.25) -> np.ndarray:
    """Anchored KF with stacked observation y_t = [M_hat_t; 0], H = [1;1]."""
    T = len(M_hat)
    if Q is None:
        Q = 1 - rho**2  # 0.19
    m = 0.0
    P = 1.0

    H = np.array([[1.0],[1.0]])  # shape (2,1)
    m_filt = np.zeros(T)

    for t in range(T):
        # Predict
        m_pred = rho * m
        P_pred = rho * P * rho + Q

        # Build stacked obs
        y = np.array([M_hat[t], 0.0])
        if t < T_anchor:
            R_aug = np.diag([R, sigma_star2_pre])
        else:
            # Hard anchor post T_anchor
            R_aug = np.diag([R, 1e-12])

        # Kalman gain for 1D state, multi-dim obs
        # S = H P_pred H' + R
        S = H @ np.array([[P_pred]]) @ H.T + R_aug  # (2,2)
        K = (np.array([[P_pred]]) @ H.T) @ np.linalg.inv(S)  # (1,2)

        # Update
        innov = y - (H.flatten()*m_pred)  # (2,)
        m = m_pred + (K @ innov)[0]
        P = (1 - (K @ H)[0,0]) * P_pred

        m_filt[t] = m
    return m_filt


# -------------------------
# 7) Propagation
# -------------------------

def propagate_distribution(pi0: np.ndarray, P_ts: list[np.ndarray]) -> np.ndarray:
    """Return array of shape (T+1, 4) of distributions over time."""
    T = len(P_ts)
    pi = np.zeros((T+1, len(RATINGS)))
    pi[0, :] = pi0
    current = pi0.copy()
    for t in range(T):
        current = current @ P_ts[t]
        pi[t+1, :] = current
    return pi


def compute_pd_series(pi_ts: np.ndarray) -> np.ndarray:
    """Return Y_t = probability mass in default at each t."""
    return pi_ts[:, IDX["D"]]


# -------------------------
# 8) Experiment Runner
# -------------------------

def run_experiment(scenario: str, method: str, N: int=N_DEFAULT, T: int=T_DEFAULT, seed: int=12345) -> dict:
    """Run scenario × method. Save CSVs and figures. Return key arrays."""
    rng = set_seed(seed + hash((scenario, method)) % 10_000)  # slight offset per run

    # Inputs
    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)
    labels0 = gen_initial_portfolio(N, pi0, rng=rng)
    P_TTC = build_P_TTC()
    betas = _build_betas()

    # Macro
    macro = gen_macro_forecasts_and_realised(scenario, T=T, rng=rng)
    M_hat = macro["M_hat"]
    M_real = macro["M_real"]

    # Filtering method selection
    if method == "raw":
        M_eff = M_real.copy()
        M_est = M_eff.copy()
    elif method == "naive":
        M_est = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)
        M_eff = M_est.copy()
    elif method == "anchored":
        M_est = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)
        M_eff = M_est.copy()
    else:
        raise ValueError("Unknown method")

    # Time-varying PIT matrices
    P_ts = [pit_overlay(P_TTC, M_eff[t], betas) for t in range(T)]

    # Propagate
    pi_ts = propagate_distribution(pi0, P_ts)
    Y_t = compute_pd_series(pi_ts)

    # TTC baseline path (neutral M=0)
    P_neutral = [P_TTC.copy() for _ in range(T)]
    pi_ttc = propagate_distribution(pi0, P_neutral)
    Y_ttc = compute_pd_series(pi_ttc)

    # Save transition matrices
    tm_rows = []
    for t in range(T):
        for i, ri in enumerate(RATINGS):
            for j, rj in enumerate(RATINGS):
                tm_rows.append({"t": t+1, "from": ri, "to": rj, "P": P_ts[t][i,j]})
    df_tm = pd.DataFrame(tm_rows)
    f_tm = OUTDIR / f"transition_matrices_{scenario}_{method}.csv"
    df_tm.to_csv(f_tm, index=False)

    # Save macro paths
    df_macro = pd.DataFrame({
        "t": np.arange(1, T+1),
        "M_forecast": M_hat,
        "M_realised": M_real,
        "M_estimate": M_est
    })
    f_macro = OUTDIR / f"macro_paths_{scenario}_{method}.csv"
    df_macro.to_csv(f_macro, index=False)

    # Save PD term structures
    df_pd = pd.DataFrame({
        "t": np.arange(0, T+1),
        "Y_t": Y_t,
        "Y_ttc": Y_ttc
    })
    f_pd = OUTDIR / f"pd_term_structures_{scenario}_{method}.csv"
    df_pd.to_csv(f_pd, index=False)

    # Figure: macro filter
    fig1 = plt.figure()
    plt.plot(np.arange(1, T+1), M_hat, label="Forecast (M̂)")
    plt.plot(np.arange(1, T+1), M_real, label="Realised M")
    plt.plot(np.arange(1, T+1), M_est, label=f"Estimate: {method}")
    plt.axhline(0.0)
    plt.xlabel("Quarter")
    plt.ylabel("Macro index (z)")
    plt.title(f"Macro filter: {scenario} × {method}")
    plt.legend()
    f_fig1 = OUTDIR / f"macro_filter_{scenario}_{method}.png"
    fig1.savefig(f_fig1, bbox_inches="tight")
    plt.close(fig1)

    return {
        "labels0": labels0,
        "P_ts": P_ts,
        "pi_ts": pi_ts,
        "Y_t": Y_t,
        "Y_ttc": Y_ttc,
        "M_hat": M_hat,
        "M_real": M_real,
        "M_est": M_est,
        "files": {
            "transition_matrices": str(f_tm),
            "macro_paths": str(f_macro),
            "pd_term_structures": str(f_pd),
            "macro_figure": str(f_fig1),
        },
    }


# -------------------------
# 9) PD Bands and Metrics
# -------------------------

def _pd_bands_figure(results_per_scn: dict, method: str, T: int=T_DEFAULT) -> str:
    """Create PD bands plot per method across scenarios. Return file path."""
    tgrid = np.arange(0, T+1)
    fig = plt.figure()
    # TTC baseline from first scenario's output
    any_key = next(iter(results_per_scn))
    Y_ttc = results_per_scn[any_key]["Y_ttc"]
    plt.plot(tgrid, Y_ttc, label="TTC baseline")
    # Scenario paths
    for scn, res in results_per_scn.items():
        plt.plot(tgrid, res["Y_t"], label=f"{scn}")
    plt.xlabel("Quarter")
    plt.ylabel("Cumulative PD (π_t[D])")
    plt.title(f"PD term-structure bands across scenarios — {method}")
    plt.legend()
    fpath = OUTDIR / f"pd_bands_{method}.png"
    fig.savefig(fpath, bbox_inches="tight")
    plt.close(fig)
    return str(fpath)


def variance_across_scenarios(Y_by_scn: dict) -> pd.DataFrame:
    """Variance of Y_t across scenarios for each horizon and method outside."""
    # Y_by_scn: {scenario: Y_t array}
    # stack to shape (n_scn, T+1)
    Ys = np.stack([v for v in Y_by_scn.values()], axis=0)
    var_t = Ys.var(axis=0, ddof=0)
    df = pd.DataFrame({
        "t": np.arange(Ys.shape[1]),
        "var_Y_t": var_t
    })
    return df


def monte_carlo_loss_volatility(scenario: str, method: str, pi0: np.ndarray, P_TTC: np.ndarray, betas: np.ndarray, 
                                base_forecasts: dict, n_rep: int=200, seed: int=777) -> dict:
    """Std of final loss fraction Y_T across forecast noise replications."""
    rng = np.random.default_rng(seed + hash((scenario, method)) % 10_000)
    T = len(base_forecasts["M_hat"])
    YT = np.zeros(n_rep)
    for r in range(n_rep):
        # Realised from forecast + noise (+ pandemic rebound if relevant)
        gdp_f = base_forecasts["gdp_forecast"]
        un_f = base_forecasts["unemp_forecast"]
        gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)
        un_r = un_f + rng.normal(0.0, 0.2, size=T)
        if scenario.lower().startswith("pandemic") and T >= 4:
            gdp_r[3] += 2.5
            un_r[3] -= 0.8
        M_hat = macro_index_from_gdp_unemp(gdp_f, un_f)
        M_real = macro_index_from_gdp_unemp(gdp_r, un_r)

        # Method
        if method == "raw":
            M_eff = M_real
        elif method == "naive":
            M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)
        elif method == "anchored":
            M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)
        else:
            raise ValueError("Unknown method")

        P_ts = [pit_overlay(P_TTC, M_eff[t], betas) for t in range(T)]
        pi_ts = propagate_distribution(pi0, P_ts)
        YT[r] = pi_ts[-1, IDX["D"]]
    return {"mean": float(YT.mean()), "std": float(YT.std(ddof=0))}


# -------------------------
# 10) Main
# -------------------------

def main():
    seed = 20250821
    rng = set_seed(seed)

    scenarios = ["baseline", "stress", "pandemic"]
    methods = ["raw", "naive", "anchored"]
    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)
    P_TTC = build_P_TTC()
    betas = _build_betas()

    # Run experiments
    all_results = {m:{} for m in methods}
    file_registry = []

    for m in methods:
        for scn in scenarios:
            res = run_experiment(scn, m, N=N_DEFAULT, T=T_DEFAULT, seed=seed)
            all_results[m][scn] = res
            files = res["files"]
            file_registry.append({
                "scenario": scn,
                "method": m,
                **files
            })

    # PD bands per method and variance across scenarios
    var_tables = []
    pd_band_files = []
    for m in methods:
        # Bands figure
        f = _pd_bands_figure(all_results[m], m, T=T_DEFAULT)
        pd_band_files.append({"method": m, "pd_bands_figure": f})

        # Variance table
        Y_by_scn = {scn: all_results[m][scn]["Y_t"] for scn in scenarios}
        df_var = variance_across_scenarios(Y_by_scn)
        df_var.insert(0, "method", m)
        var_tables.append(df_var)

    df_var_all = pd.concat(var_tables, ignore_index=True)
    f_var = OUTDIR / "variance_Y_across_scenarios.csv"
    df_var_all.to_csv(f_var, index=False)

    # Monte Carlo lifetime loss volatility per scenario × method
    mc_rows = []
    for m in methods:
        for scn in scenarios:
            base_forecasts = gen_macro_forecasts_and_realised(scn, T=T_DEFAULT, rng=rng)
            mc = monte_carlo_loss_volatility(scn, m, pi0, P_TTC, betas, base_forecasts, n_rep=200, seed=seed)
            mc_rows.append({"scenario": scn, "method": m, "YT_mean": mc["mean"], "YT_std": mc["std"]})
    df_mc = pd.DataFrame(mc_rows)
    f_mc = OUTDIR / "lifetime_loss_volatility_mc.csv"
    df_mc.to_csv(f_mc, index=False)

    # Summary metrics file combining tables
    df_summary = pd.DataFrame({
        "file": [
            str(f_var),
            str(f_mc),
        ],
        "description": [
            "Variance of Y_t across scenarios by horizon and method",
            "Lifetime loss volatility (std of Y_T) across 200 MC reps per scenario×method",
        ]
    })
    f_summary = OUTDIR / "summary_metrics.csv"
    df_summary.to_csv(f_summary, index=False)

    # Append band figures to registry and write a registry CSV
    for item in pd_band_files:
        file_registry.append({"scenario": "all", "method": item["method"], "macro_paths": "", "transition_matrices": "", "pd_term_structures": "", "macro_figure": "", "pd_bands_figure": item["pd_bands_figure"]})
    df_registry = pd.DataFrame(file_registry)
    f_registry = OUTDIR / "file_registry.csv"
    df_registry.to_csv(f_registry, index=False)

    # Return a small dict of paths for printing
    return {
        "registry": str(f_registry),
        "variance_table": str(f_var),
        "mc_table": str(f_mc),
        "summary": str(f_summary),
        "example_macro": all_results["naive"]["baseline"]["files"]["macro_paths"],
        "example_pd": all_results["anchored"]["pandemic"]["files"]["pd_term_structures"],
    }


# Execute main and print heads
paths = main()

print("Saved files (selected):")
for k, v in paths.items():
    print(f" - {k}: {v}")

print("\nExample heads:")
df_macro = pd.read_csv(paths["example_macro"])
df_pd = pd.read_csv(paths["example_pd"])

print("\nmacro_paths (baseline × naive) head:")
print(df_macro.head(6))

print("\npd_term_structures (pandemic × anchored) head:")
print(df_pd.head(6))
